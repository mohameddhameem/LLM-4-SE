{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohameddhameem/LLM-4-SE/blob/main/Experiment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c68ee97",
      "metadata": {
        "id": "9c68ee97"
      },
      "source": [
        "# CoDET-M4 Dataset Analysis Report\n",
        "## Code Quality and Maintainability Metrics Analysis\n",
        "\n",
        "**Dataset**: [CoDET-M4 on Hugging Face](https://huggingface.co/datasets/DaniilOr/CoDET-M4)\n",
        "\n",
        "This analysis examines code quality metrics and characteristics from the CoDET-M4 dataset, which contains measures of code maintainability, style, and complexity across multiple code samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76801e03",
      "metadata": {
        "id": "76801e03"
      },
      "outputs": [],
      "source": [
        "# Load required libraries and dataset\n",
        "!pip install -q datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CoDET-M4 dataset\n",
        "dataset = load_dataset(\"DaniilOr/CoDET-M4\")\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039c55f8",
      "metadata": {
        "id": "039c55f8"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing - Expand nested features and prepare for analysis\n",
        "# Set SAMPLE_SIZE to None to use entire dataset, or specify a number to limit samples\n",
        "SAMPLE_SIZE = None\n",
        "dfs = {}\n",
        "\n",
        "# Process each split\n",
        "for split_name, split_data in dataset.items():\n",
        "    # Use entire dataset if SAMPLE_SIZE is None or 0\n",
        "    if SAMPLE_SIZE is None or SAMPLE_SIZE == 0:\n",
        "        sample_size = len(split_data)\n",
        "    else:\n",
        "        sample_size = min(SAMPLE_SIZE, len(split_data))\n",
        "\n",
        "    sample_data = split_data.select(range(sample_size))\n",
        "    df = sample_data.to_pandas()\n",
        "\n",
        "    # Expand nested 'features' column into separate columns\n",
        "    if 'features' in df.columns:\n",
        "        features_df = pd.json_normalize(df['features'])\n",
        "        df = pd.concat([df.drop('features', axis=1), features_df], axis=1)\n",
        "\n",
        "    # Remove large text columns not needed for analysis\n",
        "    df = df.drop(columns=[col for col in ['code', 'cleaned_code'] if col in df.columns])\n",
        "    dfs[split_name] = df\n",
        "\n",
        "# Identify column types for analysis\n",
        "df_train = dfs['train']\n",
        "numeric_features = ['avgFunctionLength', 'avgIdentifierLength', 'avgLineLength',\n",
        "                    'emptyLinesDensity', 'functionDefinitionDensity', 'maintainabilityIndex',\n",
        "                    'maxDecisionTokens', 'whiteSpaceRatio']\n",
        "numeric_features = [f for f in numeric_features if f in df_train.columns]\n",
        "text_features = [col for col in df_train.columns if df_train[col].dtype == 'object']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e3cbcb9",
      "metadata": {
        "id": "8e3cbcb9"
      },
      "source": [
        "## Dataset Overview & Composition\n",
        "\n",
        "The CoDET-M4 dataset contains code quality metrics across multiple dimensions. Below is the distribution of samples in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9318dd",
      "metadata": {
        "id": "1c9318dd"
      },
      "outputs": [],
      "source": [
        "# Pie charts make data composition immediately intuitive - everyone gets the 'whole vs parts' concept at a glance.\n",
        "# This quickly shows stakeholders whether we have balanced data or if certain splits dominate, which affects model training strategy.\n",
        "split_sizes = {split: len(data) for split, data in dfs.items()}\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(split_sizes)))\n",
        "wedges, texts, autotexts = ax.pie(split_sizes.values(), labels=split_sizes.keys(), autopct='%1.1f%%',\n",
        "                                    colors=colors, startangle=90)\n",
        "ax.set_title('CoDET-M4 Dataset Distribution', fontsize=14, fontweight='bold')\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('black')\n",
        "    autotext.set_fontweight('bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea2e6901",
      "metadata": {
        "id": "ea2e6901"
      },
      "source": [
        "## Code Quality Metrics Analysis\n",
        "\n",
        "This section examines the distribution of key code quality metrics across the dataset. Each metric represents a different dimension of code quality and maintainability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739d6d6b",
      "metadata": {
        "id": "739d6d6b"
      },
      "outputs": [],
      "source": [
        "# Histograms reveal data skewness and outliers better than any table - you immediately see if a metric is normally distributed or heavily skewed.\n",
        "# This helps us spot data quality issues early and decide on preprocessing strategies (log transforms, outlier handling) before modeling.\n",
        "if numeric_features:\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(numeric_features):\n",
        "        axes[idx].hist(df_train[feature].dropna(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_title(f'{feature}', fontweight='bold', fontsize=11)\n",
        "        axes[idx].set_xlabel('Value')\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "    for idx in range(len(numeric_features), len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    fig.suptitle('Distribution of Code Quality Metrics', fontsize=16, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9abd03",
      "metadata": {
        "id": "ff9abd03"
      },
      "source": [
        "## Feature Correlations\n",
        "\n",
        "Understanding relationships between code quality metrics helps identify which metrics move together and reinforce each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f730fa0a",
      "metadata": {
        "id": "f730fa0a"
      },
      "outputs": [],
      "source": [
        "# Heatmaps let us spot correlated features instantly - those strong reds or blues jump out, but wall of numbers would be missed.\n",
        "# Identifying redundant metrics saves us from feature engineering waste and helps explain model behaviors to the business.\n",
        "if numeric_features:\n",
        "    correlation_matrix = df_train[numeric_features].corr()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    im = ax.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
        "\n",
        "    ax.set_xticks(np.arange(len(numeric_features)))\n",
        "    ax.set_yticks(np.arange(len(numeric_features)))\n",
        "    ax.set_xticklabels(numeric_features, rotation=45, ha='right', fontsize=10)\n",
        "    ax.set_yticklabels(numeric_features, fontsize=10)\n",
        "\n",
        "    cbar = plt.colorbar(im, ax=ax)\n",
        "    cbar.set_label('Correlation Coefficient', rotation=270, labelpad=20)\n",
        "\n",
        "    for i in range(len(numeric_features)):\n",
        "        for j in range(len(numeric_features)):\n",
        "            ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
        "                   ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
        "\n",
        "    ax.set_title('Code Quality Metrics Correlation Matrix', fontweight='bold', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11442b6e",
      "metadata": {
        "id": "11442b6e"
      },
      "source": [
        "## Code Content Analysis\n",
        "\n",
        "Analysis of the text content characteristics - code and implementation details in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14e0b7e",
      "metadata": {
        "id": "f14e0b7e"
      },
      "outputs": [],
      "source": [
        "# Understanding text lengths upfront prevents downstream surprises - extreme length variations can break models that aren't prepared for it.\n",
        "# This tells us if we need truncation strategies or if certain code samples are vastly different in complexity, affecting data stratification decisions.\n",
        "if text_features:\n",
        "    fig, axes = plt.subplots(1, len(text_features), figsize=(5*len(text_features), 5))\n",
        "    if len(text_features) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, feature in enumerate(text_features):\n",
        "        lengths = df_train[feature].astype(str).str.len()\n",
        "        axes[idx].hist(lengths, bins=50, color='darkgreen', edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_title(f'{feature}', fontweight='bold')\n",
        "        axes[idx].set_xlabel('Character Count')\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "    fig.suptitle('Text Content Length Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7033b1c9",
      "metadata": {
        "id": "7033b1c9"
      },
      "source": [
        "## Key Insights & Summary\n",
        "\n",
        "Summary statistics of the analyzed dataset provide a quick reference for the primary metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c2b370",
      "metadata": {
        "id": "96c2b370"
      },
      "outputs": [],
      "source": [
        "# Summary statistics for key metrics\n",
        "summary_stats = []\n",
        "for metric in numeric_features:\n",
        "    summary_stats.append({\n",
        "        'Metric': metric,\n",
        "        'Mean': f\"{df_train[metric].mean():.2f}\",\n",
        "        'Median': f\"{df_train[metric].median():.2f}\",\n",
        "        'Std Dev': f\"{df_train[metric].std():.2f}\",\n",
        "        'Min': f\"{df_train[metric].min():.2f}\",\n",
        "        'Max': f\"{df_train[metric].max():.2f}\"\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9f8376",
      "metadata": {
        "id": "2b9f8376"
      },
      "source": [
        "## Deep Dive: Categorical Variables Analysis\n",
        "\n",
        "Understanding the distribution and impact of programming languages, AI models, and data sources is crucial for interpreting code quality patterns across different contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162e5e1f",
      "metadata": {
        "id": "162e5e1f"
      },
      "outputs": [],
      "source": [
        "# Categorical Variables Overview\n",
        "categorical_cols = ['language', 'model', 'source']\n",
        "categorical_summary = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_train.columns:\n",
        "        categorical_summary[col] = df_train[col].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63ae230",
      "metadata": {
        "id": "e63ae230"
      },
      "outputs": [],
      "source": [
        "# Horizontal bar charts are the workhorse for category comparisons - they scale well and labels stay readable even with many categories.\n",
        "# This distribution directly impacts our ability to draw reliable conclusions per language/model; severe imbalance means certain groups might be underrepresented.\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, col in enumerate(categorical_cols):\n",
        "    if col in df_train.columns:\n",
        "        value_counts = df_train[col].value_counts().head(15)  # Top 15 for readability\n",
        "\n",
        "        axes[idx].barh(range(len(value_counts)), value_counts.values, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_yticks(range(len(value_counts)))\n",
        "        axes[idx].set_yticklabels(value_counts.index, fontsize=9)\n",
        "        axes[idx].set_xlabel('Count', fontweight='bold')\n",
        "        axes[idx].set_title(f'{col.upper()} Distribution\\n(Top {min(15, len(value_counts))} shown)', fontweight='bold', fontsize=11)\n",
        "        axes[idx].grid(True, alpha=0.3, axis='x')\n",
        "        axes[idx].invert_yaxis()\n",
        "\n",
        "        # Add count labels\n",
        "        for i, v in enumerate(value_counts.values):\n",
        "            axes[idx].text(v + 10, i, str(v), va='center', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e56175",
      "metadata": {
        "id": "17e56175"
      },
      "outputs": [],
      "source": [
        "# Boxplots + heatmaps give us the two-level view we need: first, which category has the best/worst quality (boxplot), then why (the metric breakdown via heatmap).\n",
        "# This drives actionable insights for product teams - if Python outperforms Java, they need to know if it's better function structure, lower complexity, or something else entirely.\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
        "\n",
        "metric_to_analyze = 'maintainabilityIndex'\n",
        "\n",
        "for row, col in enumerate(categorical_cols):\n",
        "    if col in df_train.columns:\n",
        "        # Top categories\n",
        "        top_categories = df_train[col].value_counts().head(10).index\n",
        "        df_filtered = df_train[df_train[col].isin(top_categories)]\n",
        "\n",
        "        # Boxplot\n",
        "        grouped_data = [df_filtered[df_filtered[col] == cat][metric_to_analyze].dropna()\n",
        "                        for cat in top_categories]\n",
        "        bp = axes[row, 0].boxplot(grouped_data, labels=top_categories, patch_artist=True)\n",
        "\n",
        "        for patch in bp['boxes']:\n",
        "            patch.set_facecolor('#87CEEB')\n",
        "\n",
        "        axes[row, 0].set_xticklabels(top_categories, rotation=45, ha='right', fontsize=9)\n",
        "        axes[row, 0].set_ylabel('Maintainability Index', fontweight='bold')\n",
        "        axes[row, 0].set_title(f'Maintainability by {col.upper()} (Top 10)', fontweight='bold')\n",
        "        axes[row, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Average metrics by category\n",
        "        avg_metrics = df_filtered.groupby(col)[numeric_features].mean().loc[top_categories]\n",
        "\n",
        "        # Heatmap\n",
        "        im = axes[row, 1].imshow(avg_metrics.T.values, cmap='RdYlGn', aspect='auto')\n",
        "        axes[row, 1].set_xticks(range(len(top_categories)))\n",
        "        axes[row, 1].set_xticklabels(top_categories, rotation=45, ha='right', fontsize=9)\n",
        "        axes[row, 1].set_yticks(range(len(numeric_features)))\n",
        "        axes[row, 1].set_yticklabels(numeric_features, fontsize=8)\n",
        "        axes[row, 1].set_title(f'Quality Metrics Heatmap by {col.upper()}', fontweight='bold')\n",
        "\n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=axes[row, 1])\n",
        "        cbar.set_label('Average Value', rotation=270, labelpad=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255ed254",
      "metadata": {
        "id": "255ed254"
      },
      "outputs": [],
      "source": [
        "# Tables work here because stakeholders often want exact numbers for their reports - they need the specific maintenance score for Python vs Java.\n",
        "# Executive dashboards live on these aggregated numbers; having them formatted and sortable by key metrics makes the business case clear and defensible.\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "\n",
        "for idx, col in enumerate(categorical_cols):\n",
        "    if col in df_train.columns:\n",
        "        top_cats = df_train[col].value_counts().head(10).index\n",
        "        summary_data = []\n",
        "\n",
        "        for cat in top_cats:\n",
        "            cat_data = df_train[df_train[col] == cat]\n",
        "            summary_data.append({\n",
        "                col.capitalize(): cat,\n",
        "                'Count': f\"{len(cat_data):,}\",\n",
        "                'Maintain.': f\"{cat_data['maintainabilityIndex'].mean():.1f}\",\n",
        "                'Func Len': f\"{cat_data['avgFunctionLength'].mean():.1f}\",\n",
        "                'Line Len': f\"{cat_data['avgLineLength'].mean():.1f}\",\n",
        "                'Empty%': f\"{cat_data['emptyLinesDensity'].mean():.2f}\"\n",
        "            })\n",
        "\n",
        "        summary_table = pd.DataFrame(summary_data)\n",
        "\n",
        "        axes[idx].axis('tight')\n",
        "        axes[idx].axis('off')\n",
        "        table = axes[idx].table(cellText=summary_table.values,\n",
        "                               colLabels=summary_table.columns,\n",
        "                               cellLoc='center', loc='center',\n",
        "                               colWidths=[0.25, 0.12, 0.12, 0.12, 0.12, 0.12])\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(9)\n",
        "        table.scale(1, 1.8)\n",
        "\n",
        "        # Color header\n",
        "        for i in range(len(summary_table.columns)):\n",
        "            table[(0, i)].set_facecolor('#3498db')\n",
        "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "        axes[idx].set_title(f'{col.upper()} - Quality Metrics Summary (Top 10)',\n",
        "                           fontweight='bold', fontsize=11, pad=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffaf5e3a",
      "metadata": {
        "id": "ffaf5e3a"
      },
      "outputs": [],
      "source": [
        "# Stacked categories matter more than individual ones - a language might be fine in isolation but problematic when paired with a specific model.\n",
        "# This reveals interaction effects that single-variable analysis misses, helping prioritize which combinations to invest in improving first.\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "combinations = [\n",
        "    ('language', 'model'),\n",
        "    ('language', 'source'),\n",
        "    ('model', 'source')\n",
        "]\n",
        "\n",
        "for idx, (col1, col2) in enumerate(combinations):\n",
        "    if col1 in df_train.columns and col2 in df_train.columns:\n",
        "        combo_data = df_train.groupby([col1, col2]).size().reset_index(name='count').sort_values('count', ascending=False).head(10)\n",
        "\n",
        "        # Create labels\n",
        "        labels = [f\"{row[0][:12]}\\n{row[1][:12]}\" for row in combo_data[[col1, col2]].values]\n",
        "        values = combo_data['count'].values\n",
        "\n",
        "        axes[idx].bar(range(len(values)), values, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_xticks(range(len(values)))\n",
        "        axes[idx].set_xticklabels(labels, fontsize=8, rotation=45, ha='right')\n",
        "        axes[idx].set_ylabel('Count', fontweight='bold')\n",
        "        axes[idx].set_title(f'Top 10: {col1.upper()} × {col2.upper()}', fontweight='bold', fontsize=11)\n",
        "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for i, v in enumerate(values):\n",
        "            axes[idx].text(i, v + 20, str(v), ha='center', fontweight='bold', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d414b41",
      "metadata": {
        "id": "6d414b41"
      },
      "outputs": [],
      "source": [
        "# Heatmaps are perfect for showing the full cross-tabulation grid at once - empty cells (white) jump out immediately where we have data gaps.\n",
        "# Identifying sparsity in certain combinations tells us where we might have reliability issues in our conclusions or where we need to collect more data.\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "combinations = [\n",
        "    ('language', 'model'),\n",
        "    ('language', 'source'),\n",
        "    ('model', 'source')\n",
        "]\n",
        "\n",
        "for idx, (col1, col2) in enumerate(combinations):\n",
        "    if col1 in df_train.columns and col2 in df_train.columns:\n",
        "        # Get top 10 for each dimension\n",
        "        top_col1 = df_train[col1].value_counts().head(10).index\n",
        "        top_col2 = df_train[col2].value_counts().head(10).index\n",
        "\n",
        "        df_filtered = df_train[df_train[col1].isin(top_col1) & df_train[col2].isin(top_col2)]\n",
        "        crosstab = pd.crosstab(df_filtered[col1], df_filtered[col2])\n",
        "\n",
        "        # Heatmap\n",
        "        im = axes[idx].imshow(crosstab.values, cmap='YlOrRd', aspect='auto')\n",
        "        axes[idx].set_xticks(range(len(crosstab.columns)))\n",
        "        axes[idx].set_yticks(range(len(crosstab.index)))\n",
        "        axes[idx].set_xticklabels(crosstab.columns, rotation=45, ha='right', fontsize=9)\n",
        "        axes[idx].set_yticklabels(crosstab.index, fontsize=9)\n",
        "        axes[idx].set_xlabel(col2.capitalize(), fontweight='bold')\n",
        "        axes[idx].set_ylabel(col1.capitalize(), fontweight='bold')\n",
        "        axes[idx].set_title(f'{col1.upper()} × {col2.upper()}\\n(Top 10 each)', fontweight='bold')\n",
        "\n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=axes[idx])\n",
        "        cbar.set_label('Count', rotation=270, labelpad=15)\n",
        "\n",
        "        # Add text annotations\n",
        "        for i in range(len(crosstab.index)):\n",
        "            for j in range(len(crosstab.columns)):\n",
        "                text = axes[idx].text(j, i, int(crosstab.values[i, j]),\n",
        "                                     ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20bffd5",
      "metadata": {
        "id": "c20bffd5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}